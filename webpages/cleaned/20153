20153
www.ics.uci.edu/~skong2/lr_bilinear.html
nonlinear, bilinear, and beyond - Shu Kong (Aimery) - UC Irvine - Computer Vision nonlinear, bilinear, and beyond Low-rank Bilinear Pooling for Fine-grained Classification Shu Kong, Charless Fowlkes Last update: March 23, 2017. Pooling second-order local feature statistics to form a high-dimensional bilinear feature has been shown to achieve state-of-the-art performance on a variety of fine-grained classification tasks. To address the computational demands of high feature dimensionality, we propose to represent the covariance features as a matrix and apply a low-rank bilinear classifier. The resulting classifier can be evaluated without explicitly computing the bilinear feature map which allows for a large reduction in the compute time as well as decreasing the effective number of parameters to be learned. To further compress the model, we propose classifier co-decomposition that factorizes the collection of bilinear classifiers into a common factor and compact per-class terms. The co-decomposition idea can be deployed through two convolutional layers and trained in an end-to-end architecture. We suggest a simple yet effective initialization that avoids explicitly first training and factorizing the larger bilinear classifiers. Through extensive experiments, we show that our model achieves state-of-the-art performance on several public datasets for fine-grained classification trained with only category labels. Importantly, our final model is an order of magnitude smaller than the recently proposed compact bilinear model, and three orders smaller than the standard bilinear CNN model. keywords: weakly supervised learning, fine-grained classification, bilinear model, bilinear classifier, low-rank, compact model, decomposition, tensorial data, second order statistics, covariance matrix, pooling, etc. Reference S. Kong, C. Fowlkes, "Low-rank Bilinear Pooling for Fine-Grained Classification", CVPR, 2017. [project page] [technical report] [abstract] [demo] [model] [poster] [slides] Update checklist creating github page; [available] quick training using caffe, including matlab files for initialization; [available] hyperparameter study by low-rank and co-decomposition on the classifier parameters; [available] three methods of visualization; [available] fine-tuning the network using matconvnet; [TODO] others...